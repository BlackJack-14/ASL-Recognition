{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2771b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6254e3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|██████████| 30/30 [38:06<00:00, 76.21s/it]\n",
      "Processing test: 100%|██████████| 27/27 [01:16<00:00,  2.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Auto-labeling complete. Check asl_yolo_autolabel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# WARNING: This auto-labels data using MediaPipe hand detection (useful if you don't already have bbox labels).\n",
    "# It saves labels in YOLO format (class_index x_center y_center width height) normalized [0..1].\n",
    "\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATASET_ROOT = Path(r\"C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\ASL_MERGED\")  # change if needed\n",
    "OUT_YOLO = Path(\"asl_yolo_autolabel\")  # output dataset with images and labels\n",
    "OUT_YOLO.mkdir(exist_ok=True)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "# We'll create train/images, train/labels, val/images, val/labels using your existing train/test split\n",
    "for split in [\"train\", \"test\"]:\n",
    "    images_out = OUT_YOLO / split / \"images\"\n",
    "    labels_out = OUT_YOLO / split / \"labels\"\n",
    "    images_out.mkdir(parents=True, exist_ok=True)\n",
    "    labels_out.mkdir(parents=True, exist_ok=True)\n",
    "    src_split = DATASET_ROOT / split\n",
    "    if not src_split.exists():\n",
    "        print(\"Missing split:\", src_split)\n",
    "        continue\n",
    "    for cls in tqdm(list(src_split.iterdir()), desc=f\"Processing {split}\"):\n",
    "        if not cls.is_dir():\n",
    "            continue\n",
    "        cls_name = cls.name\n",
    "        # class index mapping: map class letter -> numeric index (you can define mapping)\n",
    "        # Create a consistent mapping file later (data.yaml)\n",
    "        for imf in cls.glob(\"*\"):\n",
    "            if imf.suffix.lower() not in [\".jpg\", \".jpeg\", \".png\"]:\n",
    "                continue\n",
    "            img = cv2.imread(str(imf))\n",
    "            if img is None:\n",
    "                continue\n",
    "            h, w = img.shape[:2]\n",
    "            # run mediapipe\n",
    "            image_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            results = hands.process(image_rgb)\n",
    "            if results.multi_hand_landmarks:\n",
    "                # compute bounding box of landmarks\n",
    "                all_x = []\n",
    "                all_y = []\n",
    "                for lm in results.multi_hand_landmarks[0].landmark:\n",
    "                    all_x.append(lm.x)\n",
    "                    all_y.append(lm.y)\n",
    "                x_min = max(0, min(all_x))\n",
    "                x_max = min(1, max(all_x))\n",
    "                y_min = max(0, min(all_y))\n",
    "                y_max = min(1, max(all_y))\n",
    "                # convert to pixel coords\n",
    "                x1 = int(x_min * w)\n",
    "                x2 = int(x_max * w)\n",
    "                y1 = int(y_min * h)\n",
    "                y2 = int(y_max * h)\n",
    "                # expand bbox slightly to include whole hand\n",
    "                pad_w = int((x2 - x1) * 0.2)\n",
    "                pad_h = int((y2 - y1) * 0.2)\n",
    "                x1 = max(0, x1 - pad_w); x2 = min(w, x2 + pad_w)\n",
    "                y1 = max(0, y1 - pad_h); y2 = min(h, y2 + pad_h)\n",
    "                # YOLO normalized format\n",
    "                x_center = ((x1 + x2) / 2) / w\n",
    "                y_center = ((y1 + y2) / 2) / h\n",
    "                bw = (x2 - x1) / w\n",
    "                bh = (y2 - y1) / h\n",
    "                # define numeric class index for letter\n",
    "                # we will use mapping A->0, B->1, ... Z->25, plus handle 'Blank' or others if present\n",
    "                if cls_name.upper() == \"BLANK\":  # optional mapping for blank\n",
    "                    class_idx = 26\n",
    "                else:\n",
    "                    class_idx = ord(cls_name.upper()[0]) - ord(\"A\")\n",
    "                # Save image to output images\n",
    "                out_img = images_out / imf.name\n",
    "                cv2.imwrite(str(out_img), img)\n",
    "                # Save label\n",
    "                lbl_path = labels_out / (imf.stem + \".txt\")\n",
    "                with open(lbl_path, \"w\") as f:\n",
    "                    f.write(f\"{class_idx} {x_center:.6f} {y_center:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
    "            else:\n",
    "                # no hand detected: skip or optionally save as no-label\n",
    "                pass\n",
    "\n",
    "print(\"Auto-labeling complete. Check\", OUT_YOLO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc04c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created data.yaml: C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\data.yaml\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "out = Path(\"asl_yolo_autolabel\")  # or your roboflow downloaded folder\n",
    "train_images = str(out / \"train\" / \"images\")\n",
    "val_images = str(out / \"test\" / \"images\")\n",
    "\n",
    "# Define class names in order (A..Z + maybe Blank)\n",
    "CLASS_NAMES = [chr(i) for i in range(65, 91)]  # A-Z\n",
    "# If you have a 'Blank' class add it after:\n",
    "# CLASS_NAMES.append(\"Blank\")\n",
    "\n",
    "data_yaml = {\n",
    "    \"names\": CLASS_NAMES,\n",
    "    \"nc\": len(CLASS_NAMES),\n",
    "    \"train\": train_images,\n",
    "    \"val\": val_images\n",
    "}\n",
    "\n",
    "import yaml\n",
    "with open(\"data.yaml\", \"w\") as f:\n",
    "    yaml.dump(data_yaml, f, sort_keys=False)\n",
    "print(\"Created data.yaml:\", Path(\"data.yaml\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5fce0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\torch\\cuda\\__init__.py:65: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "# Using Ultralytics Python API for YOLOv8 training\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# choose a backbone: 'yolov8n.pt' (nano), 'yolov8s.pt' (small), 'yolov8m.pt' (medium) depending on GPU\n",
    "MODEL_BACKBONE = \"yolov8n.pt\"  # change to yolov8n.pt for faster experimentation\n",
    "\n",
    "# Create and train model\n",
    "model = YOLO(MODEL_BACKBONE)  # loads pretrained weights\n",
    "\n",
    "# Train: adjust epochs and batch size per your GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0437956c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.234 available  Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.222  Python-3.10.19 torch-2.10.0.dev20251030+cu130 CUDA:0 (NVIDIA GeForce RTX 5070 Ti Laptop GPU, 12227MiB)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=30, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=True, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=512, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=asl_yolo_nano3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\runs\\detect\\asl_yolo_nano3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=4, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=26\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    756382  ultralytics.nn.modules.head.Detect           [26, [64, 128, 256]]          \n",
      "Model summary: 129 layers, 3,015,918 parameters, 3,015,902 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access  (ping: 1.11.9 ms, read: 120.6183.2 MB/s, size: 88.7 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\asl_yolo_autolabel\\train\\labels.cache... 84582 images, 0 backgrounds, 3 corrupt: 100% ━━━━━━━━━━━━ 84582/84582  0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\asl_yolo_autolabel\\train\\images\\536a6db8-30c8-403d-85a9-d2f710469679.rgb_0000.png: ignoring corrupt image/label: Label class 26 exceeds dataset class count 26. Possible class labels are 0-25\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\asl_yolo_autolabel\\train\\images\\83f190d6-5197-4f26-b4d2-20d488d3c00f.rgb_0000.png: ignoring corrupt image/label: Label class 26 exceeds dataset class count 26. Possible class labels are 0-25\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mC:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\asl_yolo_autolabel\\train\\images\\98736e65-8739-4509-a5da-6b5ecca93383.rgb_0000.png: ignoring corrupt image/label: Label class 26 exceeds dataset class count 26. Possible class labels are 0-25\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.20.1 ms, read: 358.576.7 MB/s, size: 348.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\asl_yolo_autolabel\\test\\labels.cache... 2327 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2327/2327  0.0s\n",
      "Plotting labels to C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\runs\\detect\\asl_yolo_nano3\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 512 train, 512 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mC:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\runs\\detect\\asl_yolo_nano3\u001b[0m\n",
      "Starting training for 30 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/30      1.44G     0.9987      2.315      1.288          3        512: 100% ━━━━━━━━━━━━ 5287/5287 10.2it/s 8:390.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 6.9it/s 10.6s0.1s\n",
      "                   all       2327       2327      0.955      0.944      0.975      0.808\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/30      1.69G     0.8696      1.238      1.179          4        512: 100% ━━━━━━━━━━━━ 5287/5287 11.5it/s 7:41<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.5it/s 8.6s0.1s\n",
      "                   all       2327       2327      0.989      0.985      0.989      0.842\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/30      1.69G     0.8752      1.003      1.176          6        512: 100% ━━━━━━━━━━━━ 5287/5287 10.4it/s 8:280.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 6.6it/s 11.1s0.1s\n",
      "                   all       2327       2327      0.991       0.99      0.991      0.846\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/30       1.7G     0.8456     0.8629      1.157          6        512: 100% ━━━━━━━━━━━━ 5287/5287 11.7it/s 7:33<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 7.2it/s 10.1s0.1s\n",
      "                   all       2327       2327      0.991      0.993       0.99      0.869\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/30       1.7G     0.7817     0.7294      1.121         10        512: 100% ━━━━━━━━━━━━ 5287/5287 12.0it/s 7:19<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 7.4it/s 9.9s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.991      0.883\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/30       1.7G     0.7454     0.6645      1.102          8        512: 100% ━━━━━━━━━━━━ 5287/5287 10.2it/s 8:400.2ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 6.3it/s 11.6s0.2s\n",
      "                   all       2327       2327      0.993      0.994      0.991      0.887\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/30       1.7G     0.7215     0.6241       1.09          6        512: 100% ━━━━━━━━━━━━ 5287/5287 9.6it/s 9:13<0.1ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 6.4it/s 11.3s0.2s\n",
      "                   all       2327       2327      0.993      0.994      0.992      0.893\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/30       1.7G     0.7051     0.5942       1.08          6        512: 100% ━━━━━━━━━━━━ 5287/5287 10.9it/s 8:05<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.7it/s 8.4s0.1s\n",
      "                   all       2327       2327      0.993      0.994      0.992      0.895\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/30       1.7G     0.6896     0.5682      1.072          3        512: 100% ━━━━━━━━━━━━ 5287/5287 10.9it/s 8:07<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.3it/s 8.8s0.1s\n",
      "                   all       2327       2327      0.993      0.994      0.992      0.897\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/30       1.7G     0.6783     0.5479      1.067         11        512: 100% ━━━━━━━━━━━━ 5287/5287 12.4it/s 7:05<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.7it/s 8.4s0.1s\n",
      "                   all       2327       2327      0.993      0.994      0.992      0.899\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      11/30       1.7G     0.6678     0.5348      1.061          7        512: 100% ━━━━━━━━━━━━ 5287/5287 10.7it/s 8:13<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 7.5it/s 9.8s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.992      0.901\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      12/30       1.7G     0.6572     0.5174      1.056          9        512: 100% ━━━━━━━━━━━━ 5287/5287 11.2it/s 7:50<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.7it/s 8.4s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.992      0.902\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      13/30       1.7G     0.6513     0.5081      1.054          6        512: 100% ━━━━━━━━━━━━ 5287/5287 11.5it/s 7:41<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 7.4it/s 9.9s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.992      0.904\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      14/30       1.7G     0.6399     0.4937      1.047          5        512: 100% ━━━━━━━━━━━━ 5287/5287 11.0it/s 8:02<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.3it/s 8.8s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.992      0.904\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      15/30       1.7G     0.6336     0.4844      1.045          5        512: 100% ━━━━━━━━━━━━ 5287/5287 10.9it/s 8:03<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.4it/s 8.7s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.993      0.905\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      16/30       1.7G     0.6265     0.4752       1.04          4        512: 100% ━━━━━━━━━━━━ 5287/5287 11.0it/s 8:02<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 8.6it/s 8.5s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.993      0.906\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      17/30       1.7G     0.6158     0.4612      1.035          8        512: 100% ━━━━━━━━━━━━ 5287/5287 11.2it/s 7:50<0.2s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 73/73 7.9it/s 9.2s0.1s\n",
      "                   all       2327       2327      0.992      0.994      0.993      0.908\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      18/30       1.7G     0.6065     0.4503      1.038         30        512: 4% ──────────── 204/5287 15.1it/s 17.2s<5:37\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# lighter than 640\u001b[39;49;00m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# safe for 12GB VRAM\u001b[39;49;00m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhalf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43masl_yolo_nano\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\engine\\model.py:800\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mget_model(weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39myaml)\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m--> 800\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m}:\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\engine\\trainer.py:240\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 240\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\engine\\trainer.py:424\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    422\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m unwrap_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\u001b[38;5;241m.\u001b[39mloss(batch, preds)\n\u001b[0;32m    423\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 424\u001b[0m     loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\torch\\nn\\modules\\module.py:1783\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\torch\\nn\\modules\\module.py:1794\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1792\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1796\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\nn\\tasks.py:138\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;124;03mPerform forward pass of the model for either training or inference.\u001b[39;00m\n\u001b[0;32m    126\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m    (torch.Tensor): Loss if x is a dict (training), or network predictions (inference).\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[1;32m--> 138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\nn\\tasks.py:338\u001b[0m, in \u001b[0;36mBaseModel.loss\u001b[1;34m(self, batch, preds)\u001b[0m\n\u001b[0;32m    335\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_criterion()\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 338\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(preds, batch)\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\nn\\tasks.py:139\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\nn\\tasks.py:157\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\nn\\tasks.py:180\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    179\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 180\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    181\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\torch\\nn\\modules\\module.py:1783\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1783\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\torch\\nn\\modules\\module.py:1794\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1792\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1793\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1794\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1796\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1797\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:317\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    316\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 317\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    318\u001b[0m     y\u001b[38;5;241m.\u001b[39mextend(m(y[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train(\n",
    "    data=\"data.yaml\",\n",
    "    epochs=30,\n",
    "    imgsz=512,     # lighter than 640\n",
    "    batch=16,       # safe for 12GB VRAM\n",
    "    workers=4,\n",
    "    half=True,\n",
    "    name=\"asl_yolo_nano\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90b0ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\torch\\cuda\\__init__.py:65: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa679914eccb4431921ba3e3a69a9837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Start', style=ButtonStyle()), Button(button_style='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84bfa0b6288d44acbc04daea0c6d263c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='480', width='640')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Jupyter-friendly webcam + MediaPipe + YOLO display\n",
    "# Paste into one notebook cell and run.\n",
    "import threading, time, io\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# ---- load your model (change path as needed) ----\n",
    "yolo_weights = r\"./runs/detect/asl_yolo_nano3/weights/best.pt\"\n",
    "yolo = YOLO(yolo_weights)\n",
    "\n",
    "# ---- MediaPipe setup ----\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# NOTE: create Hands instance inside thread or reuse carefully\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.5)\n",
    "\n",
    "# ---- classification function (your existing logic) ----\n",
    "def classify_frame_with_mediapipe(frame):\n",
    "    h, w = frame.shape[:2]\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    res = hands.process(rgb)\n",
    "    preds = []\n",
    "    if res.multi_hand_landmarks:\n",
    "        for hand_landmarks in res.multi_hand_landmarks:\n",
    "            xs = [lm.x for lm in hand_landmarks.landmark]\n",
    "            ys = [lm.y for lm in hand_landmarks.landmark]\n",
    "            x1 = int(max(0, min(xs) * w - 0.05*w))\n",
    "            x2 = int(min(w, max(xs) * w + 0.05*w))\n",
    "            y1 = int(max(0, min(ys) * h - 0.05*h))\n",
    "            y2 = int(min(h, max(ys) * h + 0.05*h))\n",
    "            crop = frame[y1:y2, x1:x2]\n",
    "            if crop.size == 0:\n",
    "                continue\n",
    "            # Run YOLO inference on crop\n",
    "            res_yolo = yolo.predict(source=crop, conf=0.25, verbose=False)\n",
    "            boxes = res_yolo[0].boxes\n",
    "            if len(boxes) > 0:\n",
    "                top = boxes.data[0]    # [x1,y1,x2,y2,conf,cls] (Ultralytics typical ordering)\n",
    "                # Some versions place class last; keep guarded:\n",
    "                if top.shape[0] >= 6:\n",
    "                    cls_idx = int(top[5].item())\n",
    "                    conf = float(top[4].item())\n",
    "                else:\n",
    "                    cls_idx = int(top[-1].item())\n",
    "                    conf = float(top[-2].item())\n",
    "                preds.append({\"class_idx\": cls_idx, \"conf\": conf, \"bbox\": (x1,y1,x2,y2)})\n",
    "            else:\n",
    "                preds.append({\"class_idx\": None, \"conf\": 0.0, \"bbox\": (x1,y1,x2,y2)})\n",
    "    return preds\n",
    "\n",
    "# ---- UI widget ----\n",
    "image_widget = widgets.Image(format='jpeg', width=640, height=480)\n",
    "start_btn = widgets.Button(description=\"Start\", button_style='success')\n",
    "stop_btn  = widgets.Button(description=\"Stop\", button_style='danger')\n",
    "status_label = widgets.Label(value=\"Ready\")\n",
    "\n",
    "ui = widgets.HBox([start_btn, stop_btn, status_label])\n",
    "display(ui)\n",
    "display(image_widget)\n",
    "\n",
    "# ---- thread control ----\n",
    "_stop_event = threading.Event()\n",
    "_capture_thread = None\n",
    "\n",
    "def _frame_to_jpeg_bytes(frame):\n",
    "    # Convert BGR -> RGB -> JPEG bytes\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil = Image.fromarray(rgb)\n",
    "    buff = io.BytesIO()\n",
    "    pil.save(buff, format='JPEG', quality=70)\n",
    "    return buff.getvalue()\n",
    "\n",
    "def _capture_loop():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        status_label.value = \"Error: cannot open webcam\"\n",
    "        return\n",
    "    status_label.value = \"Running...\"\n",
    "    frame_idx = 0\n",
    "    try:\n",
    "        while not _stop_event.is_set():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                status_label.value = \"Frame read failed\"\n",
    "                break\n",
    "\n",
    "            # Run mediapipe + yolo and draw annotations\n",
    "            try:\n",
    "                preds = classify_frame_with_mediapipe(frame)\n",
    "            except Exception as e:\n",
    "                # protect loop from occasional inference errors\n",
    "                preds = []\n",
    "                # you can log e if needed\n",
    "            for p in preds:\n",
    "                x1,y1,x2,y2 = p[\"bbox\"]\n",
    "                cls = p[\"class_idx\"]\n",
    "                conf = p[\"conf\"]\n",
    "                label = f\"{chr(65+cls)} {conf:.2f}\" if cls is not None else \"Unknown\"\n",
    "                cv2.rectangle(frame, (x1,y1), (x2,y2), (0,255,0), 2)\n",
    "                cv2.putText(frame, label, (max(0,x1), max(10,y1-10)), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255,255,255), 2)\n",
    "\n",
    "            # Update widget\n",
    "            image_widget.value = _frame_to_jpeg_bytes(frame)\n",
    "            frame_idx += 1\n",
    "\n",
    "            # tiny sleep to yield to notebook (tune if needed)\n",
    "            time.sleep(0.02)\n",
    "\n",
    "    finally:\n",
    "        # cleanup\n",
    "        try:\n",
    "            cap.release()\n",
    "        except Exception:\n",
    "            pass\n",
    "        status_label.value = \"Stopped\"\n",
    "\n",
    "def start_capture(_=None):\n",
    "    global _capture_thread, _stop_event\n",
    "    if _capture_thread and _capture_thread.is_alive():\n",
    "        status_label.value = \"Already running\"\n",
    "        return\n",
    "    _stop_event.clear()\n",
    "    _capture_thread = threading.Thread(target=_capture_loop, daemon=True)\n",
    "    _capture_thread.start()\n",
    "\n",
    "def stop_capture(_=None):\n",
    "    _stop_event.set()\n",
    "\n",
    "start_btn.on_click(start_capture)\n",
    "stop_btn.on_click(stop_capture)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6451b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 114000\n",
      "Total batches: 446\n",
      "Using n_jobs= 12\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c7c0b67afe44d9a24d48ffcf3a0d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/446 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted: 86864 Failed: 27136\n"
     ]
    }
   ],
   "source": [
    "# Batch-per-task joblib version (Jupyter-friendly)\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from pathlib import Path\n",
    "import cv2, mediapipe as mp, numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "DATASET_ROOT = Path(r\"C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\ASL_MERGED\")\n",
    "ALLOWED_EXT = {\".jpg\", \".jpeg\", \".png\"}\n",
    "RESIZE_TO = (640, 480)\n",
    "MIN_CONF = 0.5\n",
    "\n",
    "def iter_image_label_pairs(root: Path):\n",
    "    for split in [\"train\", \"test\"]:\n",
    "        split_dir = root / split\n",
    "        if not split_dir.exists():\n",
    "            continue\n",
    "        for cls in sorted(split_dir.iterdir()):\n",
    "            if not cls.is_dir():\n",
    "                continue\n",
    "            cls_idx = ord(cls.name.upper()[0]) - ord(\"A\")\n",
    "            for imgf in cls.iterdir():\n",
    "                if imgf.suffix.lower() in ALLOWED_EXT:\n",
    "                    yield imgf, cls_idx\n",
    "\n",
    "def process_batch(batch, resize_to=None, min_det_conf=MIN_CONF):\n",
    "    \"\"\"\n",
    "    batch: list of (Path, label)\n",
    "    returns list of results like (feat, label) or (\"FAILED\", path, reason)\n",
    "    \"\"\"\n",
    "    mp_hands = mp.solutions.hands\n",
    "    out = []\n",
    "    with mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=min_det_conf) as hands:\n",
    "        for img_path, label in batch:\n",
    "            img = cv2.imread(str(img_path))\n",
    "            if img is None:\n",
    "                out.append((\"FAILED\", str(img_path), \"imread_failed\"))\n",
    "                continue\n",
    "            if resize_to:\n",
    "                img = cv2.resize(img, resize_to)\n",
    "            rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            res = hands.process(rgb)\n",
    "            if res and res.multi_hand_landmarks:\n",
    "                lm = res.multi_hand_landmarks[0]\n",
    "                feat = []\n",
    "                for l in lm.landmark:\n",
    "                    feat.extend([l.x, l.y, l.z])\n",
    "                out.append((feat, label))\n",
    "            else:\n",
    "                out.append((\"FAILED\", str(img_path), \"no_landmarks\"))\n",
    "    return out\n",
    "\n",
    "# prepare pairs and form batch-list for workers\n",
    "pairs = list(iter_image_label_pairs(DATASET_ROOT))\n",
    "print(\"Total images:\", len(pairs))\n",
    "\n",
    "BATCH_SIZE = 256   # tune: 128-1024 depending on memory\n",
    "batches = [pairs[i:i+BATCH_SIZE] for i in range(0, len(pairs), BATCH_SIZE)]\n",
    "print(\"Total batches:\", len(batches))\n",
    "\n",
    "n_jobs = max(1, cpu_count() - 20)  # keep some cores free\n",
    "print(\"Using n_jobs=\", n_jobs)\n",
    "\n",
    "# run in parallel: each job gets a whole batch and creates a single Hands instance\n",
    "results = Parallel(n_jobs=n_jobs, backend=\"loky\")(\n",
    "    delayed(process_batch)(batch, RESIZE_TO, MIN_CONF) for batch in tqdm(batches)\n",
    ")\n",
    "\n",
    "# flatten results\n",
    "flat = [item for sub in results for item in sub]\n",
    "\n",
    "# separate successes/failures (same as before)\n",
    "X_list, y_list, failed = [], [], []\n",
    "for r in flat:\n",
    "    if isinstance(r, tuple) and r and r[0] == \"FAILED\":\n",
    "        failed.append((r[1], r[2]))\n",
    "    else:\n",
    "        feat, lbl = r\n",
    "        X_list.append(feat); y_list.append(lbl)\n",
    "\n",
    "print(\"Extracted:\", len(X_list), \"Failed:\", len(failed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d726c190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using X_list / y_list from notebook memory\n",
      "Shape X: (86864, 63) y: (86864,)\n",
      "Class distribution: Counter({3: 4730, 18: 4600, 6: 3700, 10: 3694, 5: 3626, 22: 3606, 9: 3581, 21: 3480, 7: 3462, 17: 3447, 24: 3403, 20: 3351, 8: 3289, 16: 3246, 14: 3203, 25: 3196, 11: 3164, 19: 3157, 4: 3122, 1: 3110, 2: 3054, 23: 3009, 0: 2975, 15: 2897, 12: 2641, 13: 2121})\n",
      "Train samples: 73834 Val samples: 13030\n",
      "Training RandomForest...\n",
      "Validation accuracy: 0.9873\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       446\n",
      "           1       1.00      1.00      1.00       467\n",
      "           2       0.99      1.00      1.00       458\n",
      "           3       0.99      0.98      0.98       710\n",
      "           4       1.00      1.00      1.00       468\n",
      "           5       0.99      0.99      0.99       544\n",
      "           6       0.98      1.00      0.99       555\n",
      "           7       1.00      0.98      0.99       519\n",
      "           8       1.00      0.99      0.99       493\n",
      "           9       1.00      0.99      0.99       537\n",
      "          10       0.99      0.99      0.99       554\n",
      "          11       1.00      0.99      1.00       475\n",
      "          12       0.98      0.99      0.98       396\n",
      "          13       1.00      0.96      0.98       318\n",
      "          14       0.99      0.99      0.99       480\n",
      "          15       0.99      0.97      0.98       435\n",
      "          16       0.99      1.00      0.99       487\n",
      "          17       0.98      0.96      0.97       517\n",
      "          18       0.97      1.00      0.98       690\n",
      "          19       1.00      0.98      0.99       474\n",
      "          20       0.95      0.96      0.95       503\n",
      "          21       0.97      0.98      0.97       522\n",
      "          22       0.99      0.99      0.99       541\n",
      "          23       0.97      0.99      0.98       451\n",
      "          24       0.99      1.00      1.00       511\n",
      "          25       0.99      0.99      0.99       479\n",
      "\n",
      "    accuracy                           0.99     13030\n",
      "   macro avg       0.99      0.99      0.99     13030\n",
      "weighted avg       0.99      0.99      0.99     13030\n",
      "\n",
      "Saved RF model to landmark_classifier.joblib\n",
      "Saved feature cache to landmark_features.npz\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Train RF from cached features or in-memory X_list,y_list and save model\n",
    "# Run this cell to train and save landmark_classifier.joblib\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "# Configuration\n",
    "CACHE_FILE = Path(\"landmark_features.npz\")   # optional cache of features (X,y)\n",
    "OUTPUT_RF = Path(\"landmark_classifier.joblib\")\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.15\n",
    "\n",
    "# Try to load cached features first\n",
    "if CACHE_FILE.exists():\n",
    "    print(\"Loading cached features from\", CACHE_FILE)\n",
    "    arr = np.load(CACHE_FILE)\n",
    "    X = arr[\"X\"]\n",
    "    y = arr[\"y\"]\n",
    "else:\n",
    "    # fallback: check if X_list / y_list exist in global namespace (from earlier cell)\n",
    "    if \"X_list\" in globals() and \"y_list\" in globals():\n",
    "        print(\"Using X_list / y_list from notebook memory\")\n",
    "        X = np.array(X_list, dtype=np.float32)\n",
    "        y = np.array(y_list, dtype=np.int32)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"No cached features found and X_list/y_list not in memory.\\n\"\n",
    "            \"Run the landmark extraction cell first (the batch-per-task extraction), or save features to 'landmark_features.npz'.\"\n",
    "        )\n",
    "\n",
    "print(\"Shape X:\", X.shape, \"y:\", y.shape)\n",
    "print(\"Class distribution:\", Counter(y))\n",
    "\n",
    "# Train/Val split (safe fallback if stratify fails)\n",
    "try:\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y)\n",
    "except Exception as e:\n",
    "    print(\"Stratified split failed (likely class imbalance). Doing random split.\")\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=None)\n",
    "\n",
    "print(\"Train samples:\", X_train.shape[0], \"Val samples:\", X_val.shape[0])\n",
    "\n",
    "# Train RandomForest\n",
    "clf = RandomForestClassifier(n_estimators=200, n_jobs=-1, random_state=RANDOM_STATE)\n",
    "print(\"Training RandomForest...\")\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation accuracy: {acc:.4f}\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_val, y_pred, zero_division=0))\n",
    "\n",
    "# Save model\n",
    "joblib.dump(clf, OUTPUT_RF)\n",
    "print(\"Saved RF model to\", OUTPUT_RF)\n",
    "\n",
    "# Optionally re-save features for later runs\n",
    "np.savez_compressed(CACHE_FILE, X=X, y=y)\n",
    "print(\"Saved feature cache to\", CACHE_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24c525a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO (may take a second)...\n",
      "Loaded RF from landmark_classifier.joblib\n",
      "RF accuracy on images where MP found landmarks: 0.9935539321014182\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99        92\n",
      "           1       1.00      1.00      1.00        89\n",
      "           2       1.00      1.00      1.00        92\n",
      "           3       0.96      1.00      0.98        82\n",
      "           4       1.00      0.97      0.99        76\n",
      "           5       1.00      1.00      1.00        90\n",
      "           6       0.99      1.00      0.99        98\n",
      "           7       1.00      1.00      1.00        97\n",
      "           8       1.00      1.00      1.00        91\n",
      "           9       1.00      0.99      0.99        91\n",
      "          10       0.98      1.00      0.99        92\n",
      "          11       1.00      1.00      1.00        93\n",
      "          12       0.99      1.00      0.99        85\n",
      "          13       1.00      1.00      1.00        93\n",
      "          14       1.00      1.00      1.00        96\n",
      "          15       0.99      1.00      0.99        91\n",
      "          16       0.99      0.99      0.99        88\n",
      "          17       1.00      1.00      1.00        88\n",
      "          18       0.97      1.00      0.98        91\n",
      "          19       0.99      0.98      0.98        95\n",
      "          20       1.00      0.97      0.98        89\n",
      "          21       0.98      1.00      0.99        83\n",
      "          22       1.00      0.98      0.99        94\n",
      "          23       1.00      0.99      0.99        83\n",
      "          24       1.00      0.99      0.99        82\n",
      "          25       1.00      0.99      0.99        86\n",
      "\n",
      "    accuracy                           0.99      2327\n",
      "   macro avg       0.99      0.99      0.99      2327\n",
      "weighted avg       0.99      0.99      0.99      2327\n",
      "\n",
      "YOLO accuracy (images with detection): 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       100\n",
      "           1       1.00      1.00      1.00       100\n",
      "           2       1.00      1.00      1.00       100\n",
      "           3       1.00      1.00      1.00       100\n",
      "           4       1.00      1.00      1.00       100\n",
      "           5       1.00      1.00      1.00       100\n",
      "           6       1.00      1.00      1.00       100\n",
      "           7       1.00      1.00      1.00       100\n",
      "           8       1.00      1.00      1.00       100\n",
      "           9       1.00      1.00      1.00       100\n",
      "          10       1.00      1.00      1.00       100\n",
      "          11       1.00      1.00      1.00       100\n",
      "          12       1.00      1.00      1.00       100\n",
      "          13       1.00      1.00      1.00       100\n",
      "          14       1.00      1.00      1.00       100\n",
      "          15       1.00      1.00      1.00       100\n",
      "          16       1.00      1.00      1.00       100\n",
      "          17       1.00      1.00      1.00       100\n",
      "          18       1.00      1.00      1.00       100\n",
      "          19       1.00      1.00      1.00       100\n",
      "          20       1.00      1.00      1.00       100\n",
      "          21       1.00      1.00      1.00       100\n",
      "          22       1.00      1.00      1.00       100\n",
      "          23       1.00      1.00      1.00       100\n",
      "          24       1.00      1.00      1.00       100\n",
      "          25       1.00      1.00      1.00       100\n",
      "\n",
      "    accuracy                           1.00      2600\n",
      "   macro avg       1.00      1.00      1.00      2600\n",
      "weighted avg       1.00      1.00      1.00      2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Optional — compare YOLO vs RF on test images (image-level)\n",
    "# WARNING: may be slow if many images. Runs MediaPipe once per image.\n",
    "\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import joblib, json\n",
    "\n",
    "# Config — change paths if needed\n",
    "YOLO_WEIGHTS = \"./runs/detect/asl_yolo_nano3/weights/best.pt\"  # change if different\n",
    "RF_MODEL = \"landmark_classifier.joblib\"\n",
    "TEST_ROOT = Path(r\"C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\ASL_MERGED\\test\")\n",
    "\n",
    "# Load models\n",
    "print(\"Loading YOLO (may take a second)...\")\n",
    "yolo = YOLO(YOLO_WEIGHTS)\n",
    "clf = joblib.load(RF_MODEL)\n",
    "print(\"Loaded RF from\", RF_MODEL)\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.5)\n",
    "\n",
    "gt = []\n",
    "yolo_preds = []\n",
    "rf_preds = []\n",
    "\n",
    "NO_DET = -1  # we'll mark missed detections as -1\n",
    "\n",
    "for cls in sorted(TEST_ROOT.iterdir()):\n",
    "    if not cls.is_dir(): continue\n",
    "    gt_cls = ord(cls.name.upper()[0]) - ord(\"A\")\n",
    "    for imf in cls.iterdir():\n",
    "        if imf.suffix.lower() not in [\".jpg\",\".png\",\".jpeg\"]: continue\n",
    "        img = cv2.imread(str(imf))\n",
    "        if img is None: continue\n",
    "\n",
    "        # YOLO prediction (take top box if any)\n",
    "        r = yolo.predict(source=img, conf=0.25, imgsz=512, verbose=False)[0]\n",
    "        if r.boxes is not None and len(r.boxes) > 0:\n",
    "            # choose highest-conf box\n",
    "            try:\n",
    "                conf = r.boxes.conf.cpu().numpy().ravel()\n",
    "                cls_idxs = r.boxes.cls.cpu().numpy().ravel().astype(int)\n",
    "                best = int(np.argmax(conf))\n",
    "                yolo_pred = int(cls_idxs[best])\n",
    "            except Exception:\n",
    "                data = r.boxes.data.cpu().numpy()\n",
    "                best = int(np.argmax(data[:,4]))\n",
    "                yolo_pred = int(data[best,5])\n",
    "        else:\n",
    "            yolo_pred = NO_DET\n",
    "\n",
    "        # RF prediction via MediaPipe on full frame (if landmarks found)\n",
    "        rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        res = hands.process(rgb)\n",
    "        if res and res.multi_hand_landmarks:\n",
    "            lm = res.multi_hand_landmarks[0]\n",
    "            feat = [v for p in lm.landmark for v in (p.x, p.y, p.z)]\n",
    "            X = np.array(feat, dtype=np.float32).reshape(1,-1)\n",
    "            rf_pred = int(clf.predict(X)[0])\n",
    "        else:\n",
    "            rf_pred = NO_DET\n",
    "\n",
    "        gt.append(gt_cls)\n",
    "        yolo_preds.append(yolo_pred)\n",
    "        rf_preds.append(rf_pred)\n",
    "\n",
    "# Convert to arrays and print simple reports (skipping NO_DET where appropriate)\n",
    "import numpy as np\n",
    "gt = np.array(gt)\n",
    "yolo_preds = np.array(yolo_preds)\n",
    "rf_preds = np.array(rf_preds)\n",
    "\n",
    "# Evaluate only where RF produced a prediction\n",
    "mask_rf = rf_preds != NO_DET\n",
    "if mask_rf.sum() > 0:\n",
    "    print(\"RF accuracy on images where MP found landmarks:\", accuracy_score(gt[mask_rf], rf_preds[mask_rf]))\n",
    "    print(classification_report(gt[mask_rf], rf_preds[mask_rf], zero_division=0))\n",
    "else:\n",
    "    print(\"RF made no predictions (MediaPipe failed on all images)\")\n",
    "\n",
    "# Evaluate YOLO where it predicted\n",
    "mask_y = yolo_preds != NO_DET\n",
    "if mask_y.sum() > 0:\n",
    "    print(\"YOLO accuracy (images with detection):\", accuracy_score(gt[mask_y], yolo_preds[mask_y]))\n",
    "    print(classification_report(gt[mask_y], yolo_preds[mask_y], zero_division=0))\n",
    "else:\n",
    "    print(\"YOLO made no predictions (unexpected)\")\n",
    "\n",
    "# Cleanup\n",
    "hands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59809317",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "# Load trained YOLO\n",
    "yolo = YOLO(\"./runs/detect/asl_yolo_nano3/weights/best.pt\")\n",
    "\n",
    "# iterate test images and collect predictions vs labels\n",
    "gt = []\n",
    "preds = []\n",
    "\n",
    "test_root = Path(r\"C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\ASL_MERGED\\test\")\n",
    "class_map = {chr(i): i-65 for i in range(65,91)}  # 'A':0...\n",
    "\n",
    "for cls in sorted(test_root.iterdir()):\n",
    "    if not cls.is_dir():\n",
    "        continue\n",
    "    cls_idx = ord(cls.name.upper()[0]) - ord(\"A\")\n",
    "    for imf in cls.iterdir():\n",
    "        if imf.suffix.lower() not in [\".jpg\",\".png\",\".jpeg\"]:\n",
    "            continue\n",
    "        img = cv2.imread(str(imf))\n",
    "        if img is None:\n",
    "            continue\n",
    "        # simple approach: run yolo on full image and take top class prediction (if any)\n",
    "        r = yolo.predict(source=img, conf=0.25, verbose=False)\n",
    "        if len(r[0].boxes) > 0:\n",
    "            top = r[0].boxes.data[0]\n",
    "            pred_cls = int(top[5].item())\n",
    "        else:\n",
    "            pred_cls = -1  # no pred\n",
    "        gt.append(cls_idx)\n",
    "        preds.append(pred_cls)\n",
    "\n",
    "# filter out images where pred == -1 as needed or include as 'no-detection'\n",
    "labels = list(range(26))\n",
    "cm = confusion_matrix(gt, preds, labels=labels)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[chr(i) for i in range(65,91)])\n",
    "disp.plot(xticks_rotation=90, cmap=\"Blues\")\n",
    "plt.title(\"YOLO Detection Confusion Matrix (Test Set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdac4f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.222  Python-3.10.19 torch-2.10.0.dev20251030+cu130 CPU (AMD Ryzen 9 8940HX with Radeon Graphics)\n",
      "Model summary (fused): 72 layers, 3,010,718 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mPyTorch:\u001b[0m starting from 'runs\\detect\\asl_yolo_nano3\\weights\\best.pt' with input shape (1, 3, 512, 512) BCHW and output shape(s) (1, 30, 5376) (11.8 MB)\n",
      "\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m starting export with onnx 1.19.1 opset 22...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m slimming with onnxslim 0.1.72...\n",
      "\u001b[34m\u001b[1mONNX:\u001b[0m export success  3.2s, saved as 'runs\\detect\\asl_yolo_nano3\\weights\\best.onnx' (11.7 MB)\n",
      "\n",
      "Export complete (3.4s)\n",
      "Results saved to \u001b[1mC:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\runs\\detect\\asl_yolo_nano3\\weights\u001b[0m\n",
      "Predict:         yolo predict task=detect model=runs\\detect\\asl_yolo_nano3\\weights\\best.onnx imgsz=512  \n",
      "Validate:        yolo val task=detect model=runs\\detect\\asl_yolo_nano3\\weights\\best.onnx imgsz=512 data=data.yaml  \n",
      "Visualize:       https://netron.app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'runs\\\\detect\\\\asl_yolo_nano3\\\\weights\\\\best.onnx'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using ultralytics export command\n",
    "model = YOLO(\"./runs/detect/asl_yolo_nano3//weights/best.pt\")\n",
    "model.export(format=\"onnx\")  # outputs best.onnx\n",
    "# for tflite: model.export(format=\"tflite\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5b388fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.222  Python-3.10.19 torch-2.10.0.dev20251030+cu130 CUDA:0 (NVIDIA GeForce RTX 5070 Ti Laptop GPU, 12227MiB)\n",
      "Model summary (fused): 72 layers, 3,010,718 parameters, 0 gradients, 8.1 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access  (ping: 0.10.0 ms, read: 960.5938.4 MB/s, size: 256.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\asl_yolo_autolabel\\test\\labels.cache... 2327 images, 0 backgrounds, 0 corrupt: 100% ━━━━━━━━━━━━ 2327/2327  0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ━━━━━━━━━━━━ 146/146 11.1it/s 13.2s0.2s\n",
      "                   all       2327       2327      0.992      0.994      0.993      0.907\n",
      "                     A         92         92      0.998          1      0.995      0.921\n",
      "                     B         89         89      0.996          1      0.995      0.914\n",
      "                     C         92         92      0.996          1      0.995      0.941\n",
      "                     D         82         82      0.986      0.988       0.99      0.881\n",
      "                     E         76         76       0.92      0.921      0.982      0.889\n",
      "                     F         90         90      0.998          1      0.995      0.983\n",
      "                     G         98         98      0.998          1      0.995      0.863\n",
      "                     H         97         97      0.988       0.99      0.992      0.933\n",
      "                     I         91         91      0.998          1      0.995      0.943\n",
      "                     J         91         91      0.988      0.989      0.983      0.827\n",
      "                     K         92         92      0.998          1      0.995      0.949\n",
      "                     L         93         93      0.998          1      0.995      0.975\n",
      "                     M         85         85      0.998          1      0.995      0.853\n",
      "                     N         93         93      0.998          1      0.995      0.932\n",
      "                     O         96         96      0.998          1      0.995      0.874\n",
      "                     P         91         91      0.997          1      0.995      0.877\n",
      "                     Q         88         88      0.987      0.989      0.989      0.819\n",
      "                     R         88         88      0.998          1      0.995      0.935\n",
      "                     S         91         91      0.998          1      0.995       0.89\n",
      "                     T         95         95      0.998          1      0.995      0.906\n",
      "                     U         89         89      0.998          1      0.995       0.88\n",
      "                     V         83         83      0.998          1      0.995      0.954\n",
      "                     W         94         94      0.998          1      0.995      0.876\n",
      "                     X         83         83      0.986      0.988      0.987      0.919\n",
      "                     Y         82         82      0.998          1      0.995      0.964\n",
      "                     Z         86         86      0.987      0.988      0.982      0.896\n",
      "Speed: 0.8ms preprocess, 1.1ms inference, 0.0ms loss, 0.9ms postprocess per image\n",
      "Results saved to \u001b[1mC:\\Users\\rudra\\coding\\AIML\\DeepLearning\\Real-Time-ASL-Detection\\runs\\detect\\val2\u001b[0m\n",
      "ultralytics.utils.metrics.DetMetrics object with attributes:\n",
      "\n",
      "ap_class_index: array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25])\n",
      "box: ultralytics.utils.metrics.Metric object\n",
      "confusion_matrix: <ultralytics.utils.metrics.ConfusionMatrix object at 0x000002207342AA10>\n",
      "curves: ['Precision-Recall(B)', 'F1-Confidence(B)', 'Precision-Confidence(B)', 'Recall-Confidence(B)']\n",
      "curves_results: [[array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,           1,           1,           0],\n",
      "       [          1,           1,           1, ...,           1,           1,           0],\n",
      "       [          1,           1,           1, ...,           1,           1,           0],\n",
      "       ...,\n",
      "       [          1,           1,           1, ...,     0.13626,    0.068128,           0],\n",
      "       [          1,           1,           1, ...,           1,           1,           0],\n",
      "       [          1,           1,           1, ...,     0.14782,    0.073912,           0]]), 'Recall', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[    0.95833,     0.95833,      0.9674, ...,           0,           0,           0],\n",
      "       [    0.83568,     0.83568,     0.96656, ...,           0,           0,           0],\n",
      "       [    0.83258,     0.83258,     0.87373, ...,           0,           0,           0],\n",
      "       ...,\n",
      "       [    0.89617,     0.89617,     0.95369, ...,           0,           0,           0],\n",
      "       [    0.96471,     0.96471,     0.98372, ...,           0,           0,           0],\n",
      "       [    0.91892,     0.91892,     0.94362, ...,           0,           0,           0]]), 'Confidence', 'F1'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[       0.92,        0.92,     0.93686, ...,           1,           1,           1],\n",
      "       [    0.71774,     0.71775,     0.93528, ...,           1,           1,           1],\n",
      "       [    0.71318,     0.71318,     0.77578, ...,           1,           1,           1],\n",
      "       ...,\n",
      "       [       0.82,        0.82,     0.92173, ...,           1,           1,           1],\n",
      "       [    0.93182,     0.93182,     0.96797, ...,           1,           1,           1],\n",
      "       [    0.85859,     0.85859,     0.90275, ...,           1,           1,           1]]), 'Confidence', 'Precision'], [array([          0,    0.001001,    0.002002,    0.003003,    0.004004,    0.005005,    0.006006,    0.007007,    0.008008,    0.009009,     0.01001,    0.011011,    0.012012,    0.013013,    0.014014,    0.015015,    0.016016,    0.017017,    0.018018,    0.019019,     0.02002,    0.021021,    0.022022,    0.023023,\n",
      "          0.024024,    0.025025,    0.026026,    0.027027,    0.028028,    0.029029,     0.03003,    0.031031,    0.032032,    0.033033,    0.034034,    0.035035,    0.036036,    0.037037,    0.038038,    0.039039,     0.04004,    0.041041,    0.042042,    0.043043,    0.044044,    0.045045,    0.046046,    0.047047,\n",
      "          0.048048,    0.049049,     0.05005,    0.051051,    0.052052,    0.053053,    0.054054,    0.055055,    0.056056,    0.057057,    0.058058,    0.059059,     0.06006,    0.061061,    0.062062,    0.063063,    0.064064,    0.065065,    0.066066,    0.067067,    0.068068,    0.069069,     0.07007,    0.071071,\n",
      "          0.072072,    0.073073,    0.074074,    0.075075,    0.076076,    0.077077,    0.078078,    0.079079,     0.08008,    0.081081,    0.082082,    0.083083,    0.084084,    0.085085,    0.086086,    0.087087,    0.088088,    0.089089,     0.09009,    0.091091,    0.092092,    0.093093,    0.094094,    0.095095,\n",
      "          0.096096,    0.097097,    0.098098,    0.099099,      0.1001,      0.1011,      0.1021,      0.1031,      0.1041,     0.10511,     0.10611,     0.10711,     0.10811,     0.10911,     0.11011,     0.11111,     0.11211,     0.11311,     0.11411,     0.11512,     0.11612,     0.11712,     0.11812,     0.11912,\n",
      "           0.12012,     0.12112,     0.12212,     0.12312,     0.12412,     0.12513,     0.12613,     0.12713,     0.12813,     0.12913,     0.13013,     0.13113,     0.13213,     0.13313,     0.13413,     0.13514,     0.13614,     0.13714,     0.13814,     0.13914,     0.14014,     0.14114,     0.14214,     0.14314,\n",
      "           0.14414,     0.14515,     0.14615,     0.14715,     0.14815,     0.14915,     0.15015,     0.15115,     0.15215,     0.15315,     0.15415,     0.15516,     0.15616,     0.15716,     0.15816,     0.15916,     0.16016,     0.16116,     0.16216,     0.16316,     0.16416,     0.16517,     0.16617,     0.16717,\n",
      "           0.16817,     0.16917,     0.17017,     0.17117,     0.17217,     0.17317,     0.17417,     0.17518,     0.17618,     0.17718,     0.17818,     0.17918,     0.18018,     0.18118,     0.18218,     0.18318,     0.18418,     0.18519,     0.18619,     0.18719,     0.18819,     0.18919,     0.19019,     0.19119,\n",
      "           0.19219,     0.19319,     0.19419,      0.1952,      0.1962,      0.1972,      0.1982,      0.1992,      0.2002,      0.2012,      0.2022,      0.2032,      0.2042,     0.20521,     0.20621,     0.20721,     0.20821,     0.20921,     0.21021,     0.21121,     0.21221,     0.21321,     0.21421,     0.21522,\n",
      "           0.21622,     0.21722,     0.21822,     0.21922,     0.22022,     0.22122,     0.22222,     0.22322,     0.22422,     0.22523,     0.22623,     0.22723,     0.22823,     0.22923,     0.23023,     0.23123,     0.23223,     0.23323,     0.23423,     0.23524,     0.23624,     0.23724,     0.23824,     0.23924,\n",
      "           0.24024,     0.24124,     0.24224,     0.24324,     0.24424,     0.24525,     0.24625,     0.24725,     0.24825,     0.24925,     0.25025,     0.25125,     0.25225,     0.25325,     0.25425,     0.25526,     0.25626,     0.25726,     0.25826,     0.25926,     0.26026,     0.26126,     0.26226,     0.26326,\n",
      "           0.26426,     0.26527,     0.26627,     0.26727,     0.26827,     0.26927,     0.27027,     0.27127,     0.27227,     0.27327,     0.27427,     0.27528,     0.27628,     0.27728,     0.27828,     0.27928,     0.28028,     0.28128,     0.28228,     0.28328,     0.28428,     0.28529,     0.28629,     0.28729,\n",
      "           0.28829,     0.28929,     0.29029,     0.29129,     0.29229,     0.29329,     0.29429,      0.2953,      0.2963,      0.2973,      0.2983,      0.2993,      0.3003,      0.3013,      0.3023,      0.3033,      0.3043,     0.30531,     0.30631,     0.30731,     0.30831,     0.30931,     0.31031,     0.31131,\n",
      "           0.31231,     0.31331,     0.31431,     0.31532,     0.31632,     0.31732,     0.31832,     0.31932,     0.32032,     0.32132,     0.32232,     0.32332,     0.32432,     0.32533,     0.32633,     0.32733,     0.32833,     0.32933,     0.33033,     0.33133,     0.33233,     0.33333,     0.33433,     0.33534,\n",
      "           0.33634,     0.33734,     0.33834,     0.33934,     0.34034,     0.34134,     0.34234,     0.34334,     0.34434,     0.34535,     0.34635,     0.34735,     0.34835,     0.34935,     0.35035,     0.35135,     0.35235,     0.35335,     0.35435,     0.35536,     0.35636,     0.35736,     0.35836,     0.35936,\n",
      "           0.36036,     0.36136,     0.36236,     0.36336,     0.36436,     0.36537,     0.36637,     0.36737,     0.36837,     0.36937,     0.37037,     0.37137,     0.37237,     0.37337,     0.37437,     0.37538,     0.37638,     0.37738,     0.37838,     0.37938,     0.38038,     0.38138,     0.38238,     0.38338,\n",
      "           0.38438,     0.38539,     0.38639,     0.38739,     0.38839,     0.38939,     0.39039,     0.39139,     0.39239,     0.39339,     0.39439,      0.3954,      0.3964,      0.3974,      0.3984,      0.3994,      0.4004,      0.4014,      0.4024,      0.4034,      0.4044,     0.40541,     0.40641,     0.40741,\n",
      "           0.40841,     0.40941,     0.41041,     0.41141,     0.41241,     0.41341,     0.41441,     0.41542,     0.41642,     0.41742,     0.41842,     0.41942,     0.42042,     0.42142,     0.42242,     0.42342,     0.42442,     0.42543,     0.42643,     0.42743,     0.42843,     0.42943,     0.43043,     0.43143,\n",
      "           0.43243,     0.43343,     0.43443,     0.43544,     0.43644,     0.43744,     0.43844,     0.43944,     0.44044,     0.44144,     0.44244,     0.44344,     0.44444,     0.44545,     0.44645,     0.44745,     0.44845,     0.44945,     0.45045,     0.45145,     0.45245,     0.45345,     0.45445,     0.45546,\n",
      "           0.45646,     0.45746,     0.45846,     0.45946,     0.46046,     0.46146,     0.46246,     0.46346,     0.46446,     0.46547,     0.46647,     0.46747,     0.46847,     0.46947,     0.47047,     0.47147,     0.47247,     0.47347,     0.47447,     0.47548,     0.47648,     0.47748,     0.47848,     0.47948,\n",
      "           0.48048,     0.48148,     0.48248,     0.48348,     0.48448,     0.48549,     0.48649,     0.48749,     0.48849,     0.48949,     0.49049,     0.49149,     0.49249,     0.49349,     0.49449,      0.4955,      0.4965,      0.4975,      0.4985,      0.4995,      0.5005,      0.5015,      0.5025,      0.5035,\n",
      "            0.5045,     0.50551,     0.50651,     0.50751,     0.50851,     0.50951,     0.51051,     0.51151,     0.51251,     0.51351,     0.51451,     0.51552,     0.51652,     0.51752,     0.51852,     0.51952,     0.52052,     0.52152,     0.52252,     0.52352,     0.52452,     0.52553,     0.52653,     0.52753,\n",
      "           0.52853,     0.52953,     0.53053,     0.53153,     0.53253,     0.53353,     0.53453,     0.53554,     0.53654,     0.53754,     0.53854,     0.53954,     0.54054,     0.54154,     0.54254,     0.54354,     0.54454,     0.54555,     0.54655,     0.54755,     0.54855,     0.54955,     0.55055,     0.55155,\n",
      "           0.55255,     0.55355,     0.55455,     0.55556,     0.55656,     0.55756,     0.55856,     0.55956,     0.56056,     0.56156,     0.56256,     0.56356,     0.56456,     0.56557,     0.56657,     0.56757,     0.56857,     0.56957,     0.57057,     0.57157,     0.57257,     0.57357,     0.57457,     0.57558,\n",
      "           0.57658,     0.57758,     0.57858,     0.57958,     0.58058,     0.58158,     0.58258,     0.58358,     0.58458,     0.58559,     0.58659,     0.58759,     0.58859,     0.58959,     0.59059,     0.59159,     0.59259,     0.59359,     0.59459,      0.5956,      0.5966,      0.5976,      0.5986,      0.5996,\n",
      "            0.6006,      0.6016,      0.6026,      0.6036,      0.6046,     0.60561,     0.60661,     0.60761,     0.60861,     0.60961,     0.61061,     0.61161,     0.61261,     0.61361,     0.61461,     0.61562,     0.61662,     0.61762,     0.61862,     0.61962,     0.62062,     0.62162,     0.62262,     0.62362,\n",
      "           0.62462,     0.62563,     0.62663,     0.62763,     0.62863,     0.62963,     0.63063,     0.63163,     0.63263,     0.63363,     0.63463,     0.63564,     0.63664,     0.63764,     0.63864,     0.63964,     0.64064,     0.64164,     0.64264,     0.64364,     0.64464,     0.64565,     0.64665,     0.64765,\n",
      "           0.64865,     0.64965,     0.65065,     0.65165,     0.65265,     0.65365,     0.65465,     0.65566,     0.65666,     0.65766,     0.65866,     0.65966,     0.66066,     0.66166,     0.66266,     0.66366,     0.66466,     0.66567,     0.66667,     0.66767,     0.66867,     0.66967,     0.67067,     0.67167,\n",
      "           0.67267,     0.67367,     0.67467,     0.67568,     0.67668,     0.67768,     0.67868,     0.67968,     0.68068,     0.68168,     0.68268,     0.68368,     0.68468,     0.68569,     0.68669,     0.68769,     0.68869,     0.68969,     0.69069,     0.69169,     0.69269,     0.69369,     0.69469,      0.6957,\n",
      "            0.6967,      0.6977,      0.6987,      0.6997,      0.7007,      0.7017,      0.7027,      0.7037,      0.7047,     0.70571,     0.70671,     0.70771,     0.70871,     0.70971,     0.71071,     0.71171,     0.71271,     0.71371,     0.71471,     0.71572,     0.71672,     0.71772,     0.71872,     0.71972,\n",
      "           0.72072,     0.72172,     0.72272,     0.72372,     0.72472,     0.72573,     0.72673,     0.72773,     0.72873,     0.72973,     0.73073,     0.73173,     0.73273,     0.73373,     0.73473,     0.73574,     0.73674,     0.73774,     0.73874,     0.73974,     0.74074,     0.74174,     0.74274,     0.74374,\n",
      "           0.74474,     0.74575,     0.74675,     0.74775,     0.74875,     0.74975,     0.75075,     0.75175,     0.75275,     0.75375,     0.75475,     0.75576,     0.75676,     0.75776,     0.75876,     0.75976,     0.76076,     0.76176,     0.76276,     0.76376,     0.76476,     0.76577,     0.76677,     0.76777,\n",
      "           0.76877,     0.76977,     0.77077,     0.77177,     0.77277,     0.77377,     0.77477,     0.77578,     0.77678,     0.77778,     0.77878,     0.77978,     0.78078,     0.78178,     0.78278,     0.78378,     0.78478,     0.78579,     0.78679,     0.78779,     0.78879,     0.78979,     0.79079,     0.79179,\n",
      "           0.79279,     0.79379,     0.79479,      0.7958,      0.7968,      0.7978,      0.7988,      0.7998,      0.8008,      0.8018,      0.8028,      0.8038,      0.8048,     0.80581,     0.80681,     0.80781,     0.80881,     0.80981,     0.81081,     0.81181,     0.81281,     0.81381,     0.81481,     0.81582,\n",
      "           0.81682,     0.81782,     0.81882,     0.81982,     0.82082,     0.82182,     0.82282,     0.82382,     0.82482,     0.82583,     0.82683,     0.82783,     0.82883,     0.82983,     0.83083,     0.83183,     0.83283,     0.83383,     0.83483,     0.83584,     0.83684,     0.83784,     0.83884,     0.83984,\n",
      "           0.84084,     0.84184,     0.84284,     0.84384,     0.84484,     0.84585,     0.84685,     0.84785,     0.84885,     0.84985,     0.85085,     0.85185,     0.85285,     0.85385,     0.85485,     0.85586,     0.85686,     0.85786,     0.85886,     0.85986,     0.86086,     0.86186,     0.86286,     0.86386,\n",
      "           0.86486,     0.86587,     0.86687,     0.86787,     0.86887,     0.86987,     0.87087,     0.87187,     0.87287,     0.87387,     0.87487,     0.87588,     0.87688,     0.87788,     0.87888,     0.87988,     0.88088,     0.88188,     0.88288,     0.88388,     0.88488,     0.88589,     0.88689,     0.88789,\n",
      "           0.88889,     0.88989,     0.89089,     0.89189,     0.89289,     0.89389,     0.89489,      0.8959,      0.8969,      0.8979,      0.8989,      0.8999,      0.9009,      0.9019,      0.9029,      0.9039,      0.9049,     0.90591,     0.90691,     0.90791,     0.90891,     0.90991,     0.91091,     0.91191,\n",
      "           0.91291,     0.91391,     0.91491,     0.91592,     0.91692,     0.91792,     0.91892,     0.91992,     0.92092,     0.92192,     0.92292,     0.92392,     0.92492,     0.92593,     0.92693,     0.92793,     0.92893,     0.92993,     0.93093,     0.93193,     0.93293,     0.93393,     0.93493,     0.93594,\n",
      "           0.93694,     0.93794,     0.93894,     0.93994,     0.94094,     0.94194,     0.94294,     0.94394,     0.94494,     0.94595,     0.94695,     0.94795,     0.94895,     0.94995,     0.95095,     0.95195,     0.95295,     0.95395,     0.95495,     0.95596,     0.95696,     0.95796,     0.95896,     0.95996,\n",
      "           0.96096,     0.96196,     0.96296,     0.96396,     0.96496,     0.96597,     0.96697,     0.96797,     0.96897,     0.96997,     0.97097,     0.97197,     0.97297,     0.97397,     0.97497,     0.97598,     0.97698,     0.97798,     0.97898,     0.97998,     0.98098,     0.98198,     0.98298,     0.98398,\n",
      "           0.98498,     0.98599,     0.98699,     0.98799,     0.98899,     0.98999,     0.99099,     0.99199,     0.99299,     0.99399,     0.99499,       0.996,       0.997,       0.998,       0.999,           1]), array([[          1,           1,           1, ...,           0,           0,           0],\n",
      "       [          1,           1,           1, ...,           0,           0,           0],\n",
      "       [          1,           1,           1, ...,           0,           0,           0],\n",
      "       ...,\n",
      "       [    0.98795,     0.98795,     0.98795, ...,           0,           0,           0],\n",
      "       [          1,           1,           1, ...,           0,           0,           0],\n",
      "       [    0.98837,     0.98837,     0.98837, ...,           0,           0,           0]]), 'Confidence', 'Recall']]\n",
      "fitness: 0.9074840619526826\n",
      "keys: ['metrics/precision(B)', 'metrics/recall(B)', 'metrics/mAP50(B)', 'metrics/mAP50-95(B)']\n",
      "maps: array([    0.92114,     0.91449,     0.94104,     0.88056,     0.88901,     0.98292,     0.86298,     0.93261,     0.94333,     0.82739,      0.9494,     0.97476,     0.85259,     0.93241,     0.87427,     0.87704,     0.81921,     0.93505,     0.89009,     0.90567,     0.87976,     0.95429,     0.87581,     0.91935,\n",
      "           0.96383,     0.89559])\n",
      "names: {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z'}\n",
      "nt_per_class: array([92, 89, 92, 82, 76, 90, 98, 97, 91, 91, 92, 93, 85, 93, 96, 91, 88, 88, 91, 95, 89, 83, 94, 83, 82, 86], dtype=int64)\n",
      "nt_per_image: array([92, 89, 92, 82, 76, 90, 98, 97, 91, 91, 92, 93, 85, 93, 96, 91, 88, 88, 91, 95, 89, 83, 94, 83, 82, 86], dtype=int64)\n",
      "results_dict: {'metrics/precision(B)': 0.992195256016051, 'metrics/recall(B)': 0.994327672468336, 'metrics/mAP50(B)': 0.9927251545071154, 'metrics/mAP50-95(B)': 0.9074840619526826, 'fitness': 0.9074840619526826}\n",
      "save_dir: WindowsPath('C:/Users/rudra/coding/AIML/DeepLearning/Real-Time-ASL-Detection/runs/detect/val2')\n",
      "speed: {'preprocess': 0.778899871078642, 'inference': 1.079141469689582, 'loss': 0.0005230769225575116, 'postprocess': 0.9356666093602733}\n",
      "stats: {'tp': [], 'conf': [], 'pred_cls': [], 'target_cls': [], 'target_img': []}\n",
      "task: 'detect'\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "model = YOLO(\"./runs/detect/asl_yolo_nano3/weights/best.pt\")\n",
    "# Validate against data.yaml to get mAP/precision/recall\n",
    "results = model.val(data=\"data.yaml\", imgsz=512, batch=16)\n",
    "print(results)   # prints mAP, P, R etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb220a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rudra\\.conda\\envs\\blackwell\\lib\\site-packages\\torch\\cuda\\__init__.py:65: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading YOLO model (may take a moment)...\n",
      "Loading RF model...\n",
      "Models loaded.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fa9696c95374cf6b51ca39ff9365742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Button(button_style='success', description='Start', style=ButtonStyle()), Button…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a377889a4b28446e80136cd2c70dd958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='600', width='800')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready — press Start to begin webcam inference.\n"
     ]
    }
   ],
   "source": [
    "# Realtime YOLO + MediaPipe + RF (Jupyter-safe single cell)\n",
    "import threading, time, io, sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import joblib\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# -----------------------\n",
    "# CONFIG - edit if needed\n",
    "# -----------------------\n",
    "YOLO_WEIGHTS = \"./runs/detect/asl_yolo_nano3/weights/best.pt\"   # your YOLO .pt\n",
    "RF_MODEL = \"./landmark_classifier.joblib\"                      # saved RF\n",
    "IMG_SZ = 640                       # YOLO input size\n",
    "YOLO_CONF = 0.25         # detection threshold for YOLO inference\n",
    "YOLO_HIGH_CONF = 0.85    # accept YOLO only-if-no-MP and conf >= this\n",
    "PAD_RATIO = 0.12         # padding around MP bbox when cropping for RF\n",
    "MIN_CROP_SIDE = 128      # minimum side for crop (scaled up if smaller)\n",
    "IOU_THRESH = 0.25        # IoU threshold to match YOLO box to MP hand box\n",
    "MP_MIN_CONF = 0.35       # MediaPipe min_detection_confidence (lower = more sensitive)\n",
    "SHOW_FPS = True\n",
    "\n",
    "# -----------------------\n",
    "# Load models (may take a few seconds)\n",
    "# -----------------------\n",
    "print(\"Loading YOLO model (may take a moment)...\")\n",
    "yolo = YOLO(YOLO_WEIGHTS)\n",
    "print(\"Loading RF model...\")\n",
    "clf = joblib.load(RF_MODEL)\n",
    "print(\"Models loaded.\")\n",
    "\n",
    "# Create a single MediaPipe Hands instance for reuse\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=MP_MIN_CONF)\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def _frame_to_jpeg_bytes(frame_bgr, quality=70):\n",
    "    rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "    pil = Image.fromarray(rgb)\n",
    "    buff = io.BytesIO()\n",
    "    pil.save(buff, format='JPEG', quality=quality)\n",
    "    return buff.getvalue()\n",
    "\n",
    "def iou(boxA, boxB):\n",
    "    # box: (x1,y1,x2,y2)\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "    interW = max(0, xB - xA)\n",
    "    interH = max(0, yB - yA)\n",
    "    interArea = interW * interH\n",
    "    boxAArea = max(0, boxA[2]-boxA[0]) * max(0, boxA[3]-boxA[1])\n",
    "    boxBArea = max(0, boxB[2]-boxB[0]) * max(0, boxB[3]-boxB[1])\n",
    "    union = boxAArea + boxBArea - interArea\n",
    "    return interArea / union if union > 0 else 0.0\n",
    "\n",
    "def normalize_landmarks(hand_lm):\n",
    "    \"\"\"Normalize landmarks relative to hand bounding box (0-1 range)\"\"\"\n",
    "    xs = [lm.x for lm in hand_lm.landmark]\n",
    "    ys = [lm.y for lm in hand_lm.landmark]\n",
    "    zs = [lm.z for lm in hand_lm.landmark]\n",
    "    \n",
    "    x_min, x_max = min(xs), max(xs)\n",
    "    y_min, y_max = min(ys), max(ys)\n",
    "    z_min, z_max = min(zs), max(zs)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    x_range = max(x_max - x_min, 1e-6)\n",
    "    y_range = max(y_max - y_min, 1e-6)\n",
    "    z_range = max(z_max - z_min, 1e-6)\n",
    "    \n",
    "    normalized = []\n",
    "    for lm in hand_lm.landmark:\n",
    "        norm_x = (lm.x - x_min) / x_range\n",
    "        norm_y = (lm.y - y_min) / y_range\n",
    "        norm_z = (lm.z - z_min) / z_range\n",
    "        normalized.extend([norm_x, norm_y, norm_z])\n",
    "    \n",
    "    return normalized\n",
    "\n",
    "# -----------------------\n",
    "# UI widgets\n",
    "# -----------------------\n",
    "image_widget = widgets.Image(format='jpeg', width=800, height=600)\n",
    "start_btn = widgets.Button(description=\"Start\", button_style='success')\n",
    "stop_btn  = widgets.Button(description=\"Stop\", button_style='danger')\n",
    "status = widgets.Label(value=\"Ready\")\n",
    "conf_label = widgets.Label(value=f\"YOLO_CONF={YOLO_CONF}  YOLO_HIGH_CONF={YOLO_HIGH_CONF}  IOU={IOU_THRESH}\")\n",
    "ui = widgets.VBox([widgets.HBox([start_btn, stop_btn, status]), conf_label])\n",
    "display(ui)\n",
    "display(image_widget)\n",
    "\n",
    "# -----------------------\n",
    "# Processing pipeline (one-shot MP on full frame + IoU filter)\n",
    "# -----------------------\n",
    "def _process_frame(frame):\n",
    "    H, W = frame.shape[:2]\n",
    "    \n",
    "    # 1) MediaPipe full-frame pass\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_res_full = hands.process(frame_rgb)\n",
    "    \n",
    "    mp_hands_data = []  # list of dicts with 'box', 'landmarks', 'used'\n",
    "    \n",
    "    if mp_res_full and mp_res_full.multi_hand_landmarks:\n",
    "        for hand_lm in mp_res_full.multi_hand_landmarks:\n",
    "            xs = [lm.x for lm in hand_lm.landmark]\n",
    "            ys = [lm.y for lm in hand_lm.landmark]\n",
    "            x_min = max(0, min(xs))\n",
    "            x_max = min(1, max(xs))\n",
    "            y_min = max(0, min(ys))\n",
    "            y_max = min(1, max(ys))\n",
    "            \n",
    "            x1 = int(x_min * W)\n",
    "            x2 = int(x_max * W)\n",
    "            y1 = int(y_min * H)\n",
    "            y2 = int(y_max * H)\n",
    "            \n",
    "            # Add padding\n",
    "            padx = int(0.08 * (x2 - x1 + 1))\n",
    "            pady = int(0.08 * (y2 - y1 + 1))\n",
    "            xa, ya = max(0, x1 - padx), max(0, y1 - pady)\n",
    "            xb, yb = min(W, x2 + padx), min(H, y2 + pady)\n",
    "            \n",
    "            mp_hands_data.append({\n",
    "                'box': (xa, ya, xb, yb),\n",
    "                'landmarks': hand_lm,\n",
    "                'used': False\n",
    "            })\n",
    "\n",
    "    # 2) YOLO detection\n",
    "    res = yolo.predict(source=frame, conf=YOLO_CONF, imgsz=IMG_SZ, verbose=False)[0]\n",
    "    detections = []\n",
    "    \n",
    "    # If no YOLO boxes, process all MP hands with RF\n",
    "    if res.boxes is None or len(res.boxes) == 0:\n",
    "        for hand_data in mp_hands_data:\n",
    "            xa, ya, xb, yb = hand_data['box']\n",
    "            hand_lm = hand_data['landmarks']\n",
    "            \n",
    "            # Classify via RF using normalized landmarks\n",
    "            feat = normalize_landmarks(hand_lm)\n",
    "            X = np.array(feat, dtype=np.float32).reshape(1, -1)\n",
    "            \n",
    "            try:\n",
    "                pred = int(clf.predict(X)[0])\n",
    "                proba = float(clf.predict_proba(X)[0, pred]) if hasattr(clf, \"predict_proba\") else 1.0\n",
    "                label_text = f\"RF:{chr(65+pred)} {proba:.2f}\"\n",
    "            except Exception as e:\n",
    "                label_text = \"RF:ERR\"\n",
    "            \n",
    "            cv2.rectangle(frame, (xa, ya), (xb, yb), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, label_text, (xa, max(15, ya-6)), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        \n",
    "        return frame, []\n",
    "\n",
    "    # Process YOLO detections\n",
    "    data = res.boxes.data.cpu().numpy()  # Nx6 [x1,y1,x2,y2,conf,cls]\n",
    "    idxs = np.argsort(-data[:, 4])  # sort descending by conf\n",
    "    \n",
    "    for i in idxs:\n",
    "        x1, y1, x2, y2, conf, cls_idx = data[i][:6]\n",
    "        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)\n",
    "        yolo_box = (x1, y1, x2, y2)\n",
    "\n",
    "        # Find best overlapping MP box (if any)\n",
    "        matched_mp_idx = None\n",
    "        best_iou = 0.0\n",
    "        \n",
    "        for mi, hand_data in enumerate(mp_hands_data):\n",
    "            if hand_data['used']:  # Skip already matched hands\n",
    "                continue\n",
    "            val = iou(yolo_box, hand_data['box'])\n",
    "            if val > best_iou:\n",
    "                best_iou = val\n",
    "                matched_mp_idx = mi\n",
    "\n",
    "        # Case 1: No MP match or low IoU\n",
    "        if matched_mp_idx is None or best_iou < IOU_THRESH:\n",
    "            # Accept high-confidence YOLO as fallback\n",
    "            if conf >= YOLO_HIGH_CONF:\n",
    "                label_text = f\"YOLO:{chr(65+int(cls_idx))} {conf:.2f}\"\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(frame, label_text, (x1, max(15, y1-6)), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "                detections.append((x1, y1, x2, y2, conf, int(cls_idx)))\n",
    "            continue\n",
    "\n",
    "        # Case 2: Matched to an MP hand — use RF classification\n",
    "        hand_data = mp_hands_data[matched_mp_idx]\n",
    "        hand_data['used'] = True  # Mark as used\n",
    "        \n",
    "        xa, ya, xb, yb = hand_data['box']\n",
    "        hand_lm = hand_data['landmarks']\n",
    "\n",
    "        # Classify with RF using normalized landmarks\n",
    "        feat = normalize_landmarks(hand_lm)\n",
    "        X = np.array(feat, dtype=np.float32).reshape(1, -1)\n",
    "        \n",
    "        try:\n",
    "            pred = int(clf.predict(X)[0])\n",
    "            proba = float(clf.predict_proba(X)[0, pred]) if hasattr(clf, \"predict_proba\") else 1.0\n",
    "            label_text = f\"RF:{chr(65+pred)} {proba:.2f}\"\n",
    "        except Exception:\n",
    "            # Fallback to YOLO prediction\n",
    "            label_text = f\"YOLO:{chr(65+int(cls_idx))} {conf:.2f}\"\n",
    "\n",
    "        # Draw MP box (blue) and YOLO box (green)\n",
    "        cv2.rectangle(frame, (xa, ya), (xb, yb), (255, 0, 0), 2)  # MP bbox\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # YOLO bbox\n",
    "        cv2.putText(frame, label_text, (x1, max(15, y1-6)), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "        detections.append((x1, y1, x2, y2, conf, int(cls_idx)))\n",
    "\n",
    "    # Process any unmatched MP hands (detected by MP but not by YOLO)\n",
    "    for hand_data in mp_hands_data:\n",
    "        if not hand_data['used']:\n",
    "            xa, ya, xb, yb = hand_data['box']\n",
    "            hand_lm = hand_data['landmarks']\n",
    "            \n",
    "            # Classify via RF\n",
    "            feat = normalize_landmarks(hand_lm)\n",
    "            X = np.array(feat, dtype=np.float32).reshape(1, -1)\n",
    "            \n",
    "            try:\n",
    "                pred = int(clf.predict(X)[0])\n",
    "                proba = float(clf.predict_proba(X)[0, pred]) if hasattr(clf, \"predict_proba\") else 1.0\n",
    "                label_text = f\"RF:{chr(65+pred)} {proba:.2f}\"\n",
    "            except Exception:\n",
    "                label_text = \"RF:ERR\"\n",
    "            \n",
    "            cv2.rectangle(frame, (xa, ya), (xb, yb), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, label_text, (xa, max(15, ya-6)), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2)\n",
    "\n",
    "    return frame, detections\n",
    "\n",
    "# -----------------------\n",
    "# Capture loop (threaded)\n",
    "# -----------------------\n",
    "_stop_event = threading.Event()\n",
    "_capture_thread = None\n",
    "\n",
    "def _capture_loop():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        status.value = \"Error: cannot open webcam\"\n",
    "        return\n",
    "    status.value = \"Running\"\n",
    "    prev_t = time.time()\n",
    "    try:\n",
    "        while not _stop_event.is_set():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                status.value = \"Frame read failed\"\n",
    "                break\n",
    "            ann, dets = _process_frame(frame)\n",
    "            if SHOW_FPS:\n",
    "                now = time.time()\n",
    "                fps = 1.0 / (now - prev_t) if now != prev_t else 0.0\n",
    "                prev_t = now\n",
    "                cv2.putText(ann, f\"FPS: {fps:.1f}\", (10, 30), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n",
    "            image_widget.value = _frame_to_jpeg_bytes(ann)\n",
    "            time.sleep(0.01)\n",
    "    finally:\n",
    "        try:\n",
    "            cap.release()\n",
    "        except:\n",
    "            pass\n",
    "        status.value = \"Stopped\"\n",
    "\n",
    "def start_capture(_=None):\n",
    "    global _capture_thread, _stop_event\n",
    "    if _capture_thread and _capture_thread.is_alive():\n",
    "        status.value = \"Already running\"\n",
    "        return\n",
    "    _stop_event.clear()\n",
    "    _capture_thread = threading.Thread(target=_capture_loop, daemon=True)\n",
    "    _capture_thread.start()\n",
    "\n",
    "def stop_capture(_=None):\n",
    "    _stop_event.set()\n",
    "    time.sleep(0.15)\n",
    "    status.value = \"Stopping...\"\n",
    "\n",
    "start_btn.on_click(start_capture)\n",
    "stop_btn.on_click(stop_capture)\n",
    "\n",
    "# Clean-up on kernel interrupt\n",
    "def _cleanup():\n",
    "    try:\n",
    "        hands.close()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "import atexit\n",
    "atexit.register(_cleanup)\n",
    "\n",
    "print(\"Ready — press Start to begin webcam inference.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f164da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blackwell",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
